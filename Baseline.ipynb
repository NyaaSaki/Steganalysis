{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-w9jq2Uqd8y",
        "outputId": "6e7f00c6-0fba-415c-e8ae-b024f4200dc3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-EeHbxXy_jH"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qRT8AT3Wy_Gw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: torchtext in c:\\users\\shado\\appdata\\roaming\\python\\python310\\site-packages (0.16.1)\n",
            "Requirement already satisfied: torchdata==0.7.1 in c:\\users\\shado\\appdata\\roaming\\python\\python310\\site-packages (from torchtext) (0.7.1)\n",
            "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from torchtext) (4.64.1)\n",
            "Requirement already satisfied: torch==2.1.1 in c:\\users\\shado\\appdata\\roaming\\python\\python310\\site-packages (from torchtext) (2.1.1)\n",
            "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from torchtext) (2.28.1)\n",
            "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from torchtext) (1.23.5)\n",
            "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch==2.1.1->torchtext) (3.1.2)\n",
            "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch==2.1.1->torchtext) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions in c:\\programdata\\anaconda3\\lib\\site-packages (from torch==2.1.1->torchtext) (4.4.0)\n",
            "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\lib\\site-packages (from torch==2.1.1->torchtext) (2022.11.0)\n",
            "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch==2.1.1->torchtext) (2.8.4)\n",
            "Requirement already satisfied: sympy in c:\\programdata\\anaconda3\\lib\\site-packages (from torch==2.1.1->torchtext) (1.11.1)\n",
            "Requirement already satisfied: urllib3>=1.25 in c:\\programdata\\anaconda3\\lib\\site-packages (from torchdata==0.7.1->torchtext) (1.26.14)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchtext) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchtext) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchtext) (2022.12.7)\n",
            "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->torchtext) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch==2.1.1->torchtext) (2.1.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy->torch==2.1.1->torchtext) (1.2.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchtext\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
        "import torchtext\n",
        "import torch\n",
        "from torch import nn,optim\n",
        "from time import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjC37RPGzF8G",
        "outputId": "bb3f74ca-10aa-4215-8501-4a297fbc01cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      Unnamed: 0                                               text  encoded\n",
            "0              0  The soil I’m walking over comes    from deeper...    False\n",
            "1              1  the close air of the earth whence she derived ...     True\n",
            "2              2  Lyric night of the lingering Indian Summer,\\nS...    False\n",
            "3              3  “Percussus sum sicut foenum, et aruit cor meum...    False\n",
            "4              4  I should be happy with my lot: A wife and moth...    False\n",
            "...          ...                                                ...      ...\n",
            "4789        4789  office has never really is its oldest daughter...     True\n",
            "4790        4790  soot from her car with suitcases and hugged he...     True\n",
            "4791        4791  Seeing in crowded restaurants the one you love...    False\n",
            "4792        4792  flower sweet as I can never forsake And Fortun...     True\n",
            "4793        4793  Echo that loved hid within a wood Would to her...    False\n",
            "\n",
            "[4794 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv(\"dataframe.csv\")\n",
        "print(data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-sTY6w8sPBX"
      },
      "source": [
        "Creating tokenizer and vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MDqO8gKlr_8-"
      },
      "outputs": [],
      "source": [
        "from torchtext.data import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from torch.nn.utils import rnn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(data, test_size=0.2)\n",
        "\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "def build_vocabulary(datasets):\n",
        "    for dataset in datasets:\n",
        "        #print(dataset)\n",
        "        for text in dataset['text']:\n",
        "            yield tokenizer(text)\n",
        "\n",
        "vocab = build_vocab_from_iterator(build_vocabulary([data,data]), min_freq=3, specials=[\"<UNK>\"])\n",
        "\n",
        "vocab.set_default_index(vocab[\"<UNK>\"])\n",
        "\n",
        "for l in data['text']:\n",
        "  tokens = tokenizer(l)\n",
        "  indexes = vocab(tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1N3ERPfXsS5Y"
      },
      "source": [
        "Creating Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lCTwLmc3qbY",
        "outputId": "0c39e30a-52dd-42b0-9924-b513b5a55397"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class Text(Dataset):\n",
        "  def __init__(self,df):\n",
        "    self.df = df\n",
        "    #print(df)\n",
        "\n",
        "  def pad(self,seq):\n",
        "    if len(seq)>140:\n",
        "      return seq[:140]\n",
        "    else:\n",
        "      for i in range(140-len(seq)):\n",
        "        seq.insert(0,0)\n",
        "      return seq\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df['encoded'])\n",
        "  def __getitem__(self,idx):\n",
        "    \n",
        "    return torch.tensor(self.pad(vocab(tokenizer(self.df['text'][idx])))) , int(self.df['encoded'][idx])\n",
        "\n",
        "test_loader  = DataLoader(Text(test.reset_index()) , batch_size=4)\n",
        "train_loader = DataLoader(Text(train.reset_index()) , batch_size=128)\n",
        "\n",
        "for txt , tgt in test_loader:\n",
        "  _\n",
        "  #print(txt)\n",
        "  #print(txt)\n",
        "voc = len(vocab)\n",
        "#print(tokens)\n",
        "\n",
        "print(vocab([\"<UNK>\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZA8yAqD5uNlv",
        "outputId": "8378145f-943c-440c-b1a6-e6bc4ad80aa3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      Unnamed: 0                                               text  encoded\n",
            "257          257  She took the words for a stroll and the words ...    False\n",
            "3516        3516  death. The dreadful sound of a bat s wing. Kil...     True\n",
            "4772        4772  All those ;Liquid love affairs, Blind swimmers...    False\n",
            "2330        2330  A rung’s come broken in the ladder to the mowa...    False\n",
            "1185        1185  its secret recording of life. The window. And ...     True\n",
            "...          ...                                                ...      ...\n",
            "2808        2808  Across the bridge, where in the morning blow T...    False\n",
            "395          395  ranting in the hot Highland fling their shotgu...     True\n",
            "2335        2335  Know Celia, since thou art so proud, ; 'Twas I...    False\n",
            "2328        2328  For Ethan Canin    I sat on the dock at dusk a...    False\n",
            "3790        3790  When our two souls stand up erect and strong, ...    False\n",
            "\n",
            "[959 rows x 3 columns]\n",
            "      Unnamed: 0                                               text  encoded\n",
            "2239        2239  We trekked into a far country, My friend and I...    False\n",
            "44            44  a dream fluttering your eyelids wide open in t...     True\n",
            "313          313  the rusty pantaloons and gone now. This being ...     True\n",
            "1273        1273  Beginning on a line by Silvio Rodríguez    How...    False\n",
            "1272        1272  any clerk. I wounded leg in a cake. Before pen...     True\n",
            "...          ...                                                ...      ...\n",
            "1232        1232  After Lyle Lovett    If I had a ginko tree I'd...    False\n",
            "2794        2794  The Eiffel Tower erected itself in my head,we ...    False\n",
            "1535        1535  Loving you less than life, a little less Than ...    False\n",
            "4569        4569  place it. His blackened body and succulents wh...     True\n",
            "1081        1081  He described her mouth as full of ashes. So wh...    False\n",
            "\n",
            "[3835 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "print(test)\n",
        "print(train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBASklq2sVY4"
      },
      "source": [
        "Creating RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lho9HQKX5xQ4",
        "outputId": "c8bc3725-3f1c-440d-84c9-0a194fd89e47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RNN(\n",
            "  (embedding_layer): Embedding(18658, 64)\n",
            "  (rnn): RNN(64, 32, num_layers=3, batch_first=True)\n",
            "  (linear1): Linear(in_features=32, out_features=32, bias=True)\n",
            "  (linear2): Linear(in_features=32, out_features=2, bias=True)\n",
            "  (act): ReLU()\n",
            "  (out): LogSoftmax(dim=1)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RNN, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(len(vocab),64)\n",
        "        self.rnn = nn.RNN(64,32, 3, batch_first=True)\n",
        "        self.linear1 = nn.Linear(32,32)\n",
        "        self.linear2 = nn.Linear(32,2)\n",
        "        self.act = nn.ReLU()\n",
        "        self.out = nn.LogSoftmax(dim = 1)\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.embedding_layer(X_batch)\n",
        "        output, hidden = self.rnn(embeddings)\n",
        "        return self.out(self.linear2(self.act(self.linear1(self.act(output[:,-1])))))\n",
        "\n",
        "model = RNN()\n",
        "\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Module Name                                                                           Total Parameters  Trainable Parameters # Elements in Trainable Parametrs       \n",
            "Embedding(18658, 64)                                                                  1                 1                    1,194,112                               \n",
            "RNN(64, 32, num_layers=3, batch_first=True)                                           12                12                   7,360                                   \n",
            "Linear(in_features=32, out_features=32, bias=True)                                    2                 2                    1,056                                   \n",
            "Linear(in_features=32, out_features=2, bias=True)                                     2                 2                    66                                      \n",
            "ReLU()                                                                                0                 0                    0                                       \n",
            "LogSoftmax(dim=1)                                                                     0                 0                    0                                       \n",
            "Total number of trainable parameters elements 1,202,594\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "RNN(\n",
              "  (embedding_layer): EmbeddingKet(18658, 64)\n",
              "  (rnn): RNN(64, 32, num_layers=3, batch_first=True)\n",
              "  (linear1): Linear(in_features=32, out_features=32, bias=True)\n",
              "  (linear2): Linear(in_features=32, out_features=2, bias=True)\n",
              "  (act): ReLU()\n",
              "  (out): LogSoftmax(dim=1)\n",
              ")"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from word2ket import EmbeddingKet, EmbeddingKetXS , ketify,summary\n",
        "summary(model)\n",
        "ketify(model,order = 8,rank = 4, use_EmbeddingKetXS= False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Module Name                                                                           Total Parameters  Trainable Parameters # Elements in Trainable Parametrs       \n",
            "EmbeddingKet(18658, 64)                                                               1                 1                    1,194,112                               \n",
            "RNN(64, 32, num_layers=3, batch_first=True)                                           12                12                   7,360                                   \n",
            "Linear(in_features=32, out_features=32, bias=True)                                    2                 2                    1,056                                   \n",
            "Linear(in_features=32, out_features=2, bias=True)                                     2                 2                    66                                      \n",
            "ReLU()                                                                                0                 0                    0                                       \n",
            "LogSoftmax(dim=1)                                                                     0                 0                    0                                       \n",
            "Total number of trainable parameters elements 1,202,594\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "1202594"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summary(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Xm4dx7xel-tW"
      },
      "outputs": [],
      "source": [
        "accuracy = []\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.017, momentum=0.9)\n",
        "criterion = nn.NLLLoss()\n",
        "time0 = time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUNFSJmNsXd4"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGFaOLWuCXZr",
        "outputId": "ea4031a7-e6ef-4e62-f9df-1f82601c52f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1 - Training loss: 0.08669995839397113\n",
            "Model Accuracy = 0.529718456725756\n",
            "\n",
            "Epoch 2 - Training loss: 0.08665662904580435\n",
            "Model Accuracy = 0.470281543274244\n",
            "\n",
            "Epoch 3 - Training loss: 0.08664869219064712\n",
            "Model Accuracy = 0.470281543274244\n",
            "\n",
            "Epoch 4 - Training loss: 0.08664957011739413\n",
            "Model Accuracy = 0.470281543274244\n",
            "\n",
            "Epoch 5 - Training loss: 0.0866493433713913\n",
            "Model Accuracy = 0.470281543274244\n",
            "\n",
            "Epoch 6 - Training loss: 0.08664935901761055\n",
            "Model Accuracy = 0.470281543274244\n",
            "\n",
            "Epoch 7 - Training loss: 0.08664933070540429\n",
            "Model Accuracy = 0.470281543274244\n",
            "\n",
            "Epoch 8 - Training loss: 0.08664931133389472\n",
            "Model Accuracy = 0.470281543274244\n",
            "\n",
            "Epoch 9 - Training loss: 0.08664928625027339\n",
            "Model Accuracy = 0.470281543274244\n",
            "\n",
            "Epoch 10 - Training loss: 0.08664926290512084\n",
            "Model Accuracy = 0.470281543274244\n",
            "\n",
            "Epoch 11 - Training loss: 0.08664923310279846\n",
            "Model Accuracy = 0.470281543274244\n",
            "\n",
            "Epoch 12 - Training loss: 0.08664920727411905\n",
            "Model Accuracy = 0.470281543274244\n",
            "\n",
            "Epoch 13 - Training loss: 0.08664918219049772\n",
            "Model Accuracy = 0.470281543274244\n",
            "\n",
            "Epoch 14 - Training loss: 0.08664915909369787\n",
            "Model Accuracy = 0.470281543274244\n",
            "\n",
            "Epoch 15 - Training loss: 0.08664913674195608\n",
            "Model Accuracy = 0.470281543274244\n",
            "\n",
            "Epoch 16 - Training loss: 0.08664911886056265\n",
            "Model Accuracy = 0.470281543274244\n",
            "\n",
            "Epoch 17 - Training loss: 0.08664909824728965\n",
            "Model Accuracy = 0.470281543274244\n",
            "\n",
            "Epoch 18 - Training loss: 0.08664907862742742\n",
            "Model Accuracy = 0.470281543274244\n",
            "\n",
            "Epoch 19 - Training loss: 0.0866490587592125\n",
            "Model Accuracy = 0.470281543274244\n",
            "\n",
            "Epoch 20 - Training loss: 0.08664903814593951\n",
            "Model Accuracy = 0.470281543274244\n"
          ]
        }
      ],
      "source": [
        "epochs = 20\n",
        "for e in range(epochs):\n",
        "    running_loss = 0\n",
        "    for text, tgt in train_loader:\n",
        "        # Training pass\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(text)\n",
        "\n",
        "        loss = criterion(output, tgt)\n",
        "\n",
        "        #This is where the model learns by backpropagating\n",
        "        loss.backward()\n",
        "        #And optimizes its weights here\n",
        "        optimizer.step()\n",
        "        #print(loss.item())\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    else:\n",
        "        #print(\"\\nTraining Time (in minutes) =\",(time()-time0)/60)\n",
        "        #print(output)\n",
        "        print(\"\\nEpoch {} - Training loss: {}\".format(e+1, running_loss/len(test_loader)))\n",
        "    correct_count, all_count = 0, 0\n",
        "    for images,labels in test_loader:\n",
        "      for i in range(len(labels)):\n",
        "        img = images[i].view(1,-1)\n",
        "        with torch.no_grad():\n",
        "          logps = model(img)\n",
        "\n",
        "\n",
        "        ps = torch.exp(logps)\n",
        "        probab = list(ps.numpy()[0])\n",
        "        pred_label = probab.index(max(probab))\n",
        "        true_label = labels.numpy()[i]\n",
        "        #print(pred_label)\n",
        "        if(true_label == pred_label):\n",
        "          correct_count += 1\n",
        "        all_count += 1\n",
        "\n",
        "  #print(\"Number Of Images Tested =\", all_count)\n",
        "    print(\"Model Accuracy =\", (correct_count/all_count))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZAXL3oV02zg"
      },
      "source": [
        "#Texts for testing:\n",
        "\n",
        "Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
        "\n",
        "Even my brother is not like to speak with me. They treat me like aids patent.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
