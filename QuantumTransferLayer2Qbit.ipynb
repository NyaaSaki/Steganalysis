{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-w9jq2Uqd8y",
        "outputId": "7cad9b15-3b3a-4f68-cf88-cc540cf3bf76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pennylane\n",
            "  Downloading PennyLane-0.34.0-py3-none-any.whl (1.6 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.6 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.6 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.11.4)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pennylane) (3.2.1)\n",
            "Collecting rustworkx (from pennylane)\n",
            "  Downloading rustworkx-0.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: autograd in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.6.2)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from pennylane) (0.10.2)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.4.4)\n",
            "Collecting semantic-version>=2.7 (from pennylane)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting autoray>=0.6.1 (from pennylane)\n",
            "  Downloading autoray-0.6.8-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from pennylane) (5.3.2)\n",
            "Collecting pennylane-lightning>=0.34 (from pennylane)\n",
            "  Downloading PennyLane_Lightning-0.34.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pennylane) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pennylane) (4.9.0)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.10/dist-packages (from autograd->pennylane) (0.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2024.2.2)\n",
            "Installing collected packages: semantic-version, rustworkx, autoray, pennylane-lightning, pennylane\n",
            "Successfully installed autoray-0.6.8 pennylane-0.34.0 pennylane-lightning-0.34.0 rustworkx-0.14.0 semantic-version-2.10.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pennylane"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-EeHbxXy_jH"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qRT8AT3Wy_Gw"
      },
      "outputs": [],
      "source": [
        "#!pip install torchtext\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
        "import torchtext\n",
        "import torch\n",
        "from torch import nn,optim\n",
        "from time import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjC37RPGzF8G",
        "outputId": "dcca8136-f9fb-47fc-9175-abc9c3023247"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      Unnamed: 0                                               text  encoded\n",
            "0              0  The soil I’m walking over comes    from deeper...    False\n",
            "1              1  the close air of the earth whence she derived ...     True\n",
            "2              2  Lyric night of the lingering Indian Summer,\\nS...    False\n",
            "3              3  “Percussus sum sicut foenum, et aruit cor meum...    False\n",
            "4              4  I should be happy with my lot: A wife and moth...    False\n",
            "...          ...                                                ...      ...\n",
            "4789        4789  office has never really is its oldest daughter...     True\n",
            "4790        4790  soot from her car with suitcases and hugged he...     True\n",
            "4791        4791  Seeing in crowded restaurants the one you love...    False\n",
            "4792        4792  flower sweet as I can never forsake And Fortun...     True\n",
            "4793        4793  Echo that loved hid within a wood Would to her...    False\n",
            "\n",
            "[4794 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv(\"dataframe.csv\")\n",
        "print(data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-sTY6w8sPBX"
      },
      "source": [
        "Creating tokenizer and vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "MDqO8gKlr_8-"
      },
      "outputs": [],
      "source": [
        "from torchtext.data import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from torch.nn.utils import rnn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data, test = train_test_split(data, test_size=0.8)\n",
        "train, test = train_test_split(data, test_size=0.2)\n",
        "\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "def build_vocabulary(datasets):\n",
        "    for dataset in datasets:\n",
        "        #print(dataset)\n",
        "        for text in dataset['text']:\n",
        "            yield tokenizer(text)\n",
        "\n",
        "vocab = build_vocab_from_iterator(build_vocabulary([data,data]), min_freq=3, specials=[\"<UNK>\"])\n",
        "\n",
        "vocab.set_default_index(vocab[\"<UNK>\"])\n",
        "\n",
        "for l in data['text']:\n",
        "  tokens = tokenizer(l)\n",
        "  indexes = vocab(tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1N3ERPfXsS5Y"
      },
      "source": [
        "Creating Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lCTwLmc3qbY",
        "outputId": "b140241d-410d-4fc3-8745-40b02f372e98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class Text(Dataset):\n",
        "  def __init__(self,df):\n",
        "    self.df = df\n",
        "    #print(df)\n",
        "\n",
        "  def pad(self,seq):\n",
        "    if len(seq)>140:\n",
        "      return seq[:140]\n",
        "    else:\n",
        "      for i in range(140-len(seq)):\n",
        "        seq.insert(0,0)\n",
        "      return seq\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df['encoded'])\n",
        "  def __getitem__(self,idx):\n",
        "\n",
        "    return torch.tensor(self.pad(vocab(tokenizer(self.df['text'][idx])))) , int(self.df['encoded'][idx])\n",
        "\n",
        "test_loader  = DataLoader(Text(test.reset_index()), batch_size=4)\n",
        "train_loader = DataLoader(Text(train.reset_index()),batch_size=4)\n",
        "\n",
        "for txt , tgt in test_loader:\n",
        "  _\n",
        "  #print(txt)\n",
        "  #print(txt)\n",
        "voc = len(vocab)\n",
        "#print(tokens)\n",
        "\n",
        "print(vocab([\"<UNK>\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZA8yAqD5uNlv",
        "outputId": "2000eb65-678e-42cc-dc0b-e552c00773e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      Unnamed: 0                                               text  encoded\n",
            "316          316  This is all I am to her now: a pair of legs in...    False\n",
            "2812        2812  description naturallyof their tunes such agoni...     True\n",
            "2310        2310  The ribbed black of the umbrella   is an argum...    False\n",
            "1086        1086  would get up and watch him descend the tempest...     True\n",
            "1139        1139  one alone Into his ken Or like bodies that can...     True\n",
            "...          ...                                                ...      ...\n",
            "3721        3721  around me to me before we did you play For goo...     True\n",
            "1473        1473  The sky hangs up its starry pictures: a swan, ...    False\n",
            "3378        3378  How many times these low feet staggered - Only...    False\n",
            "1541        1541  I am dead And rotten None that go return again...     True\n",
            "2522        2522  scolded us all. He imagines the sky. Like a tr...     True\n",
            "\n",
            "[192 rows x 3 columns]\n",
            "      Unnamed: 0                                               text  encoded\n",
            "1091        1091  long ago that I was hushed Egypt melted. Every...     True\n",
            "1979        1979  His whippy voice skinning its tired song off t...     True\n",
            "903          903  When I push your button you fly off the handle...    False\n",
            "2784        2784  what. True Such is my power. Who in the cloth ...     True\n",
            "2940        2940  \"And then there were three whereas before ther...    False\n",
            "...          ...                                                ...      ...\n",
            "2454        2454  Oh, oh, you will be sorry for that word! Give ...    False\n",
            "2649        2649  in the arm. Scarred by a Spring day. Only tawn...     True\n",
            "4534        4534  so far I clutch. The ravishing empress nakedin...     True\n",
            "3158        3158  The birds have flown their summer skies to the...    False\n",
            "1109        1109  for j. byrd    i am a man's head hunched in th...    False\n",
            "\n",
            "[766 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "print(test)\n",
        "print(train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dy6hk9hSBN-B"
      },
      "source": [
        "## Utils\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLWwIBW9BN-C",
        "outputId": "1ca0ed8a-3ade-492a-cd79-a497faefc2b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0, 1]\n"
          ]
        }
      ],
      "source": [
        "def bitwise_input(input,n_max = 16):\n",
        "    bits = []\n",
        "    for i in bin(input)[2:]:\n",
        "        bits.append(int(i))\n",
        "    # filling\n",
        "    for _ in range(n_max - len(bits)):\n",
        "        bits.insert(0,0)\n",
        "    return bits[:n_max]\n",
        "\n",
        "print(bitwise_input(1,2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "n_X6JppDBN-D"
      },
      "outputs": [],
      "source": [
        "def getProb(prob,nmax = 16):\n",
        "    out = []\n",
        "    for i in range(nmax):\n",
        "        out.append(prob[2**nmax-1])\n",
        "    print(out)\n",
        "    return out\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VokNOBorBN-E"
      },
      "source": [
        "# Quantum Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "GaTqOEfmBN-E"
      },
      "outputs": [],
      "source": [
        "import pennylane as qml\n",
        "n_qubits = 2\n",
        "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
        "\n",
        "@qml.qnode(dev)\n",
        "def qnode(inputs, weights):\n",
        "    #inp = bitwise_input(inputs,n_qubits)\n",
        "    #print(inputs)\n",
        "    qml.AngleEmbedding(inputs, wires=range(n_qubits))\n",
        "    qml.BasicEntanglerLayers(weights, wires=range(n_qubits))\n",
        "    return [qml.expval(qml.PauliZ(wires = i)) for i in range(n_qubits)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "sRbsk2voBN-F"
      },
      "outputs": [],
      "source": [
        "n_layers = 1\n",
        "weight_shapes = {\"weights\": (n_layers, n_qubits)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "v6g2ZLx8BN-G"
      },
      "outputs": [],
      "source": [
        "class HybridModel(torch.nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self , embeddingSize):\n",
        "        super().__init__()\n",
        "        self.memory = {}\n",
        "        self.inpFormat = torch.nn.Linear(16, n_qubits*2)\n",
        "        self.qlayer_1 = qml.qnn.TorchLayer(qnode, weight_shapes)\n",
        "        self.qlayer_2 = qml.qnn.TorchLayer(qnode, weight_shapes)\n",
        "        self.clayer = torch.nn.Linear(2*n_qubits, embeddingSize)\n",
        "\n",
        "\n",
        "\n",
        "    def __EmbeddingBag__(self,X):\n",
        "\n",
        "        if str(X) in self.memory:\n",
        "            # Check if index is Cached\n",
        "            return self.memory[str(X)].clone()\n",
        "        #Formatting Index into Binary\n",
        "        x = torch.tensor(bitwise_input(int(X) , 16))\n",
        "        #Convert Binary output into float\n",
        "        x = self.inpFormat(x.float())\n",
        "        #Split Layer into 2 seperate Quantum Circuits\n",
        "        x_1 = self.qlayer_1(x[:n_qubits-1])\n",
        "        x_2 = self.qlayer_2(x[n_qubits :])\n",
        "\n",
        "\n",
        "        #Concatnate and Resize output\n",
        "        xc = torch.cat([x_1 , x_2], axis=-1)\n",
        "        xc = self.clayer(xc.float())\n",
        "\n",
        "        #Cache Value\n",
        "        self.memory[str(X)] = xc.clone()\n",
        "        return xc\n",
        "\n",
        "    def __RecursiveBag__(self,x):\n",
        "        #print(x.dim())\n",
        "        if x.dim()==0: return self.__EmbeddingBag__(x)\n",
        "\n",
        "        Bag = []\n",
        "        for subtensor in x:\n",
        "            Bag.append(self.__RecursiveBag__(subtensor))\n",
        "        #print(Bag)\n",
        "        return torch.stack(Bag)\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.memory = {}\n",
        "        if  x.dim()==0: return self.__EmbeddingBag__(x)\n",
        "        return self.__RecursiveBag__(x)\n",
        "\n",
        "#qmodel = HybridModel(64)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "96He1EE-BN-H"
      },
      "outputs": [],
      "source": [
        "#print(qmodel)\n",
        "#print(qmodel(torch.tensor([13])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBASklq2sVY4"
      },
      "source": [
        "# Testing Hybrid Model for Consistency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "XeLyUWUrBN-I"
      },
      "outputs": [],
      "source": [
        "from random import randint\n",
        "import seaborn as sb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZdapHwbBN-J"
      },
      "source": [
        "# Declaring Hybrid Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "lho9HQKX5xQ4"
      },
      "outputs": [],
      "source": [
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RNN, self).__init__()\n",
        "        self.embedding_layer = HybridModel(64)\n",
        "        self.rnn = nn.RNN(64,32, 3, batch_first=True)\n",
        "        self.linear1 = nn.Linear(32,32)\n",
        "        self.linear2 = nn.Linear(32,2)\n",
        "        self.act = nn.ReLU()\n",
        "        self.out = nn.LogSoftmax(dim = 1)\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.embedding_layer(X_batch)\n",
        "        output, hidden = self.rnn(embeddings)\n",
        "        return self.out(self.linear2(self.act(self.linear1(self.act(output[:,-1])))))\n",
        "\n",
        "#model = RNN()\n",
        "\n",
        "#print(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "d_s33-_-kVSr"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "dbfile = open('ClassicOriginalModel', 'rb')\n",
        "model = pickle.load(dbfile)\n",
        "dbfile.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwL98-FwEWsu",
        "outputId": "1a03a91a-7155-4a4d-ad10-6df5e6eecc88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting word2ket\n",
            "  Downloading word2ket-0.0.2-py3-none-any.whl (7.7 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from word2ket) (2.1.0+cu121)\n",
            "Collecting gpytorch (from word2ket)\n",
            "  Downloading gpytorch-1.11-py3-none-any.whl (266 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.1/266.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from gpytorch->word2ket) (1.2.2)\n",
            "Collecting linear-operator>=0.5.0 (from gpytorch->word2ket)\n",
            "  Downloading linear_operator-0.5.2-py3-none-any.whl (175 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.6/175.6 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->word2ket) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->word2ket) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->word2ket) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->word2ket) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->word2ket) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->word2ket) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->word2ket) (2.1.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from linear-operator>=0.5.0->gpytorch->word2ket) (1.11.4)\n",
            "Collecting jaxtyping>=0.2.9 (from linear-operator>=0.5.0->gpytorch->word2ket)\n",
            "  Downloading jaxtyping-0.2.25-py3-none-any.whl (39 kB)\n",
            "Collecting typeguard~=2.13.3 (from linear-operator>=0.5.0->gpytorch->word2ket)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->word2ket) (2.1.5)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->gpytorch->word2ket) (1.25.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->gpytorch->word2ket) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->gpytorch->word2ket) (3.3.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->word2ket) (1.3.0)\n",
            "Installing collected packages: typeguard, jaxtyping, linear-operator, gpytorch, word2ket\n",
            "Successfully installed gpytorch-1.11 jaxtyping-0.2.25 linear-operator-0.5.2 typeguard-2.13.3 word2ket-0.0.2\n"
          ]
        }
      ],
      "source": [
        "!pip install word2ket"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSekXgzPBN-K",
        "outputId": "312c5d1a-014f-47eb-b7ec-3188a08deca6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Module Name                                                                           Total Parameters  Trainable Parameters # Elements in Trainable Parametrs       \n",
            "Embedding(18658, 64)                                                                  1                 1                    1,194,112                               \n",
            "RNN(64, 32, num_layers=3, batch_first=True)                                           12                12                   7,360                                   \n",
            "Linear(in_features=32, out_features=32, bias=True)                                    2                 2                    1,056                                   \n",
            "Linear(in_features=32, out_features=2, bias=True)                                     2                 2                    66                                      \n",
            "ReLU()                                                                                0                 0                    0                                       \n",
            "LogSoftmax(dim=1)                                                                     0                 0                    0                                       \n",
            "Total number of trainable parameters elements 1,202,594\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "1202594"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from word2ket import summary\n",
        "summary(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_NaRn2fkofp",
        "outputId": "0b820961-ff52-4675-eba7-6ef0691fee40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Module Name                                                                           Total Parameters  Trainable Parameters # Elements in Trainable Parametrs       \n",
            "Embedding(18658, 64)                                                                  1                 0                    0                                       \n",
            "RNN(64, 32, num_layers=3, batch_first=True)                                           12                0                    0                                       \n",
            "Linear(in_features=32, out_features=32, bias=True)                                    2                 0                    0                                       \n",
            "Linear(in_features=32, out_features=2, bias=True)                                     2                 0                    0                                       \n",
            "ReLU()                                                                                0                 0                    0                                       \n",
            "LogSoftmax(dim=1)                                                                     0                 0                    0                                       \n",
            "Total number of trainable parameters elements 0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for layer in model.parameters():\n",
        "  layer.requires_grad = False\n",
        "summary(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kCUsEJam3Xc",
        "outputId": "a4b50bcb-4774-42de-bd7e-4fb6529d3902"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Module Name                                                                           Total Parameters  Trainable Parameters # Elements in Trainable Parametrs       \n",
            "Linear(in_features=16, out_features=4, bias=True)                                     2                 2                    68                                      \n",
            "<Quantum Torch Layer: func=qnode>                                                     1                 1                    2                                       \n",
            "<Quantum Torch Layer: func=qnode>                                                     1                 1                    2                                       \n",
            "Linear(in_features=4, out_features=64, bias=True)                                     2                 2                    320                                     \n",
            "RNN(64, 32, num_layers=3, batch_first=True)                                           12                0                    0                                       \n",
            "Linear(in_features=32, out_features=32, bias=True)                                    2                 0                    0                                       \n",
            "Linear(in_features=32, out_features=2, bias=True)                                     2                 0                    0                                       \n",
            "ReLU()                                                                                0                 0                    0                                       \n",
            "LogSoftmax(dim=1)                                                                     0                 0                    0                                       \n",
            "Total number of trainable parameters elements 392\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "392"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.embedding_layer = HybridModel(64)\n",
        "summary(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Xm4dx7xel-tW"
      },
      "outputs": [],
      "source": [
        "accuracy = []\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.015, momentum=0.9)\n",
        "criterion = nn.NLLLoss()\n",
        "time0 = time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUNFSJmNsXd4"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ClGhRFaJBN-M"
      },
      "outputs": [],
      "source": [
        "import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGFaOLWuCXZr",
        "outputId": "e1df98c7-56dd-46e6-9965-1c74c64d0c8f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 0: 100%|██████████| 192/192 [2:14:55<00:00, 42.17s/batch, loss=0.466]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1 - Training loss: 0.7754822638817132\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/48 [02:54<?, ?items/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Accuracy = 0.7395833333333334\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "accuracy = []\n",
        "epochs = 1\n",
        "for e in range(epochs):\n",
        "    running_loss = 0\n",
        "    with tqdm.tqdm(train_loader , unit = \"batch\") as tepoch:\n",
        "      for text, tgt in tepoch:\n",
        "        tepoch.set_description(f\"Epoch {e}\")\n",
        "\n",
        "        with torch.autograd.set_detect_anomaly(True):\n",
        "          optimizer.zero_grad()\n",
        "          output = model(text)\n",
        "          loss = criterion(output, tgt)\n",
        "          loss.backward(retain_graph=True)\n",
        "\n",
        "        optimizer.step()\n",
        "        tepoch.set_postfix(loss=loss.item())\n",
        "        #print(loss.item())\n",
        "        running_loss += loss.item()\n",
        "\n",
        "      else:\n",
        "        print(\"\\nEpoch {} - Training loss: {}\".format(e+1, running_loss/len(train_loader)))\n",
        "\n",
        "\n",
        "    correct_count, all_count = 0, 0\n",
        "    with tqdm.tqdm(test_loader , unit = \"items\") as tepoch:\n",
        "      for images,labels in test_loader:\n",
        "        for i in range(len(labels)):\n",
        "          img = images[i].view(1,-1)\n",
        "          with torch.no_grad():\n",
        "            logps = model(img)\n",
        "\n",
        "          ps = torch.exp(logps)\n",
        "          probab = list(ps.numpy()[0])\n",
        "          pred_label = probab.index(max(probab))\n",
        "          true_label = labels.numpy()[i]\n",
        "\n",
        "          if(true_label == pred_label):\n",
        "            correct_count += 1\n",
        "          all_count += 1\n",
        "\n",
        "      print(\"Model Accuracy =\", (correct_count/all_count))\n",
        "      accuracy.append((correct_count/all_count))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZAXL3oV02zg"
      },
      "source": [
        "#Texts for testing:\n",
        "\n",
        "Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
        "\n",
        "Even my brother is not like to speak with me. They treat me like aids patent.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "FsCDg-tt-dFQ"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "dbfile = open('2QubitParams', 'wb')\n",
        "\n",
        "pickle.dump(model.state_dict(), dbfile)\n",
        "dbfile.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
